{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "unionwords = ['or','and']\n",
    "result=dict()\n",
    "\n",
    "def union2(p1,p2):\n",
    "    '''This def will return the union of two binary searches''' \n",
    "    L3 = [max(l1, l2) for l1, l2 in zip(p1, p2)]\n",
    "    return(L3)\n",
    "def intersection2(p1,p2):\n",
    "    '''This def will return the intersection of two binary searches'''\n",
    "    L3 = [min(l1, l2) for l1, l2 in zip(p1, p2)]\n",
    "    return(L3)\n",
    "def tokenize(search):\n",
    "    '''This def tokenize, lower csae, and stem all the strings'''\n",
    "    search_token = word_tokenize(search) #Tokenize\n",
    "#     search_token = [i.lower() for i in search_token] # Lower Case\n",
    "    search_token = [porter.stem(word) for word in search_token] #Stem\n",
    "    return(search_token)\n",
    "def extractkeys(tokens , mydict):\n",
    "    '''This token will extract the operation sumbols and match the keys of each tokens with our binary matrix'''\n",
    "    unionwords = ['or','and']\n",
    "    operations = [word for word in unionwords if word in tokens] #operation\n",
    "    keys = [word for word in tokens if word not in unionwords and word in [*mydict.keys()]] #keys\n",
    "    return(operations, keys)\n",
    "def operation(p1,p2,operation):\n",
    "    '''This def does the union and intersection, given the binary data, and the operation types (or, and)'''\n",
    "    out=list()\n",
    "    if operation == ['or']:\n",
    "        out=union2(p1,p2)\n",
    "    elif operation == ['and']:\n",
    "        out=intersection2(p1,p2)\n",
    "    return(out)\n",
    "\n",
    "\n",
    "def prepro(docs):\n",
    "    ''' This is the preprocessing module, whitch given a txt document that each new line is one document, and each line starts with the documentID. \n",
    "    This Def will return the term-document incidence matrix'''\n",
    "    infile = open(docs,'r')\n",
    "    docs=infile.readlines()\n",
    "    docs=[i.strip() for i in docs] #removing the \\n\n",
    "    docs=[i.lower() for i in docs] #Lower Case\n",
    "\n",
    "    tokenized = [word_tokenize(docs[i]) for i in range(len(docs))]\n",
    "    [i.pop(0) for i in tokenized] # Removing the ID\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    stemmed=[]\n",
    "    for i in range(len(docs)):\n",
    "        stem = [porter.stem(word) for word in tokenized[i]]\n",
    "        stemmed.append(stem)\n",
    "\n",
    "    tokens = set(x for l in stemmed for x in l)\n",
    "    mydict = dict()\n",
    "    for el in tokens:\n",
    "        mydict[el]=[stemmed[i].count(el) for i in range(len(docs))]\n",
    "    return(mydict)\n",
    "\n",
    "def process2(search,mydict):\n",
    "    '''This def, takes the operation search (i.e. patiens and drugs), and term-document incidence matrix and finds the search result. The search results are shown in a binary list, which each value of this arry represent document (starting from the 1st document to the last.)'''\n",
    "    inside=re.findall('\\((.*?)\\)',search)\n",
    "    outside = re.findall('\\)(.*?)\\(',search)\n",
    "    search_token = tokenize(search)\n",
    "    operations, search_keys = extractkeys(search_token,mydict)\n",
    "\n",
    "    if len(search_keys)==1:\n",
    "        result['operat']=mydict[search_keys[0]]\n",
    "    if len(search_keys)==2:  \n",
    "        result['operat']=operation(mydict[search_keys[0]],mydict[search_keys[1]],operations)\n",
    "\n",
    "    if len(search_keys) > 2:\n",
    "        for i in range(len(inside)):\n",
    "            tokens = tokenize(inside[i])\n",
    "            operations_inside, keys_inside = extractkeys(tokens,mydict)\n",
    "            result[i]=operation(mydict[keys_inside[0]],mydict[keys_inside[1]],operations_inside)\n",
    "        \n",
    "        #First:\n",
    "        if search_token[0]!='(':\n",
    "            result['operat']=operation(result[0], mydict[search_token[0]],[search_token[1]])\n",
    "        else:\n",
    "            result['operat']=result[0]\n",
    "        print(result['operat'])\n",
    "        #Middle:\n",
    "        \n",
    "        if outside:\n",
    "            operations_outside = [tokenize(i) for i in outside]\n",
    "            for j in range(0,len(operations_outside)):\n",
    "                result['operat'] = operation(result['operat'], result[j+1],operations_outside[j])\n",
    "        #Last:    \n",
    "        if search_token[-1]!=')':\n",
    "            result['operat']=operation(result['operat'], mydict[search_token[-1]],[search_token[-2]])\n",
    "    return(result['operat'])\n",
    "\n",
    "\n",
    "def main(docs = \"docs.txt\"):\n",
    "    search=input(\"give the operation:\")\n",
    "    mydict = prepro(docs)\n",
    "    result = process2(search,mydict)\n",
    "    return(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
